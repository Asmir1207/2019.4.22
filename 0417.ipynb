{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use numpy to load dataset\n",
    "dataset =np.loadtxt('./NFSYSU-Wu-Zhan-master/dataset/stock_2.csv',delimiter=\",\",dtype=\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6110, 10)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['index_code', 'date', 'open', 'close', 'low', 'high', 'volume',\n",
       "        'money', 'change', 'label'],\n",
       "       ['sh000001', '1990/12/20', '104.3', '104.39', '99.98', '104.39',\n",
       "        '197000', '85000', '0.044108822', '109.13'],\n",
       "       ['sh000001', '1990/12/21', '109.07', '109.13', '103.73', '109.13',\n",
       "        '28000', '16100', '0.045406648', '114.55'],\n",
       "       ['sh000001', '1990/12/24', '113.57', '114.55', '109.13', '114.55',\n",
       "        '32000', '31100', '0.049665537', '120.25'],\n",
       "       ['sh000001', '1990/12/25', '120.09', '120.25', '114.55', '120.25',\n",
       "        '15000', '6500', '0.04975993', '125.27']], dtype='<U12')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas 导入数据\n",
    "dataset_pd=pd.read_csv('./NFSYSU-Wu-Zhan-master/dataset/stock_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_code</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "      <th>money</th>\n",
       "      <th>change</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sh000001</td>\n",
       "      <td>1990/12/20</td>\n",
       "      <td>104.30</td>\n",
       "      <td>104.39</td>\n",
       "      <td>99.98</td>\n",
       "      <td>104.39</td>\n",
       "      <td>197000.0</td>\n",
       "      <td>85000.0</td>\n",
       "      <td>0.044109</td>\n",
       "      <td>109.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sh000001</td>\n",
       "      <td>1990/12/21</td>\n",
       "      <td>109.07</td>\n",
       "      <td>109.13</td>\n",
       "      <td>103.73</td>\n",
       "      <td>109.13</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>16100.0</td>\n",
       "      <td>0.045407</td>\n",
       "      <td>114.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sh000001</td>\n",
       "      <td>1990/12/24</td>\n",
       "      <td>113.57</td>\n",
       "      <td>114.55</td>\n",
       "      <td>109.13</td>\n",
       "      <td>114.55</td>\n",
       "      <td>32000.0</td>\n",
       "      <td>31100.0</td>\n",
       "      <td>0.049666</td>\n",
       "      <td>120.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sh000001</td>\n",
       "      <td>1990/12/25</td>\n",
       "      <td>120.09</td>\n",
       "      <td>120.25</td>\n",
       "      <td>114.55</td>\n",
       "      <td>120.25</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>0.049760</td>\n",
       "      <td>125.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sh000001</td>\n",
       "      <td>1990/12/26</td>\n",
       "      <td>125.27</td>\n",
       "      <td>125.27</td>\n",
       "      <td>120.25</td>\n",
       "      <td>125.27</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>53700.0</td>\n",
       "      <td>0.041746</td>\n",
       "      <td>125.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  index_code        date    open   close     low    high    volume    money  \\\n",
       "0   sh000001  1990/12/20  104.30  104.39   99.98  104.39  197000.0  85000.0   \n",
       "1   sh000001  1990/12/21  109.07  109.13  103.73  109.13   28000.0  16100.0   \n",
       "2   sh000001  1990/12/24  113.57  114.55  109.13  114.55   32000.0  31100.0   \n",
       "3   sh000001  1990/12/25  120.09  120.25  114.55  120.25   15000.0   6500.0   \n",
       "4   sh000001  1990/12/26  125.27  125.27  120.25  125.27  100000.0  53700.0   \n",
       "\n",
       "     change   label  \n",
       "0  0.044109  109.13  \n",
       "1  0.045407  114.55  \n",
       "2  0.049666  120.25  \n",
       "3  0.049760  125.27  \n",
       "4  0.041746  125.28  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas 的 数据 转成numpy的矩阵\n",
    "dataset_np = dataset_pd.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6109, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['sh000001', '1990/12/20', 104.3, 104.39, 99.98, 104.39, 197000.0,\n",
       "        85000.0, 0.044108822, 109.13],\n",
       "       ['sh000001', '1990/12/21', 109.07, 109.13, 103.73, 109.13,\n",
       "        28000.0, 16100.0, 0.045406648, 114.55],\n",
       "       ['sh000001', '1990/12/24', 113.57, 114.55, 109.13, 114.55,\n",
       "        32000.0, 31100.0, 0.049665537, 120.25],\n",
       "       ['sh000001', '1990/12/25', 120.09, 120.25, 114.55, 120.25,\n",
       "        15000.0, 6500.0, 0.04975993, 125.27],\n",
       "       ['sh000001', '1990/12/26', 125.27, 125.27, 120.25, 125.27,\n",
       "        100000.0, 53700.0, 0.041746362, 125.28]], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_np[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 切出X和y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset_np[:,2:-1].astype(np.float32)\n",
    "y = dataset_np[:,-1:].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6109, 7)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6109, 1)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(X.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 切分训练集 和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "standard = StandardScaler()\n",
    "standard.fit(X_train)\n",
    "\n",
    "X_train_standard =standard.transform(X_train)\n",
    "X_test_standard =standard.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设定超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X.shape[1]\n",
    "ouput_size = y.shape[1]\n",
    "hidden_size = 32 # 神经元的个数为32\n",
    "learning_rate = 1e-2\n",
    "EPOCH = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建模型——全连接神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size,hidden_size) #[-1,7]*[7,32]==>[-1,32]\n",
    "        self.hidden2 = nn.Linear(hidden_size,hidden_size) #[-1,32]*[32,32]\n",
    "        \n",
    "        self.predict = nn.Linear(hidden_size,ouput_size) #[-1,32]*[32,1 ]\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #x(X_train 和 X_test) == [-1,7]\n",
    "        #真正搭建网络的地方\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        \n",
    "        out = self.predict(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (hidden1): Linear(in_features=7, out_features=32, bias=True)\n",
       "  (hidden2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (predict): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化器，损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(),lr=learning_rate)\n",
    "loss_func =  nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4887, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始训练我们的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...Epoch: 50 Loss: 3003117.5\n",
      "Training...Epoch: 100 Loss: 610202.25\n",
      "Training...Epoch: 150 Loss: 147015.45\n",
      "Training...Epoch: 200 Loss: 81208.28\n",
      "Training...Epoch: 250 Loss: 40363.613\n",
      "Training...Epoch: 300 Loss: 16359.588\n",
      "Training...Epoch: 350 Loss: 7221.7354\n",
      "Training...Epoch: 400 Loss: 3858.0903\n",
      "Training...Epoch: 450 Loss: 2075.6875\n",
      "Training...Epoch: 500 Loss: 1450.6141\n",
      "-----------------------------\n",
      "Testing...Epoch: 500 Loss: 1633.3112\n",
      "-----------------------------\n",
      "Training...Epoch: 550 Loss: 1252.4307\n",
      "Training...Epoch: 600 Loss: 1118.5753\n",
      "Training...Epoch: 650 Loss: 1050.0193\n",
      "Training...Epoch: 700 Loss: 1010.1191\n",
      "Training...Epoch: 750 Loss: 978.7086\n",
      "Training...Epoch: 800 Loss: 954.46216\n",
      "Training...Epoch: 850 Loss: 940.5181\n",
      "Training...Epoch: 900 Loss: 931.4474\n",
      "Training...Epoch: 950 Loss: 924.60754\n",
      "Training...Epoch: 1000 Loss: 919.64526\n",
      "-----------------------------\n",
      "Testing...Epoch: 1000 Loss: 1324.4283\n",
      "-----------------------------\n",
      "Training...Epoch: 1050 Loss: 915.09717\n",
      "Training...Epoch: 1100 Loss: 910.7037\n",
      "Training...Epoch: 1150 Loss: 906.4278\n",
      "Training...Epoch: 1200 Loss: 902.4242\n",
      "Training...Epoch: 1250 Loss: 898.54736\n",
      "Training...Epoch: 1300 Loss: 894.86523\n",
      "Training...Epoch: 1350 Loss: 891.2015\n",
      "Training...Epoch: 1400 Loss: 887.5216\n",
      "Training...Epoch: 1450 Loss: 883.87775\n",
      "Training...Epoch: 1500 Loss: 880.2647\n",
      "-----------------------------\n",
      "Testing...Epoch: 1500 Loss: 1293.743\n",
      "-----------------------------\n",
      "Training...Epoch: 1550 Loss: 876.67334\n",
      "Training...Epoch: 1600 Loss: 872.8557\n",
      "Training...Epoch: 1650 Loss: 869.1673\n",
      "Training...Epoch: 1700 Loss: 865.46643\n",
      "Training...Epoch: 1750 Loss: 861.7295\n",
      "Training...Epoch: 1800 Loss: 857.9563\n",
      "Training...Epoch: 1850 Loss: 854.19305\n",
      "Training...Epoch: 1900 Loss: 850.4175\n",
      "Training...Epoch: 1950 Loss: 846.5985\n",
      "Training...Epoch: 2000 Loss: 842.83936\n",
      "-----------------------------\n",
      "Testing...Epoch: 2000 Loss: 1262.0083\n",
      "-----------------------------\n",
      "Training...Epoch: 2050 Loss: 838.9992\n",
      "Training...Epoch: 2100 Loss: 835.11694\n",
      "Training...Epoch: 2150 Loss: 831.3244\n",
      "Training...Epoch: 2200 Loss: 827.4434\n",
      "Training...Epoch: 2250 Loss: 823.591\n",
      "Training...Epoch: 2300 Loss: 819.7682\n",
      "Training...Epoch: 2350 Loss: 815.8347\n",
      "Training...Epoch: 2400 Loss: 811.97375\n",
      "Training...Epoch: 2450 Loss: 808.03796\n",
      "Training...Epoch: 2500 Loss: 804.1417\n",
      "-----------------------------\n",
      "Testing...Epoch: 2500 Loss: 1229.5867\n",
      "-----------------------------\n",
      "Training...Epoch: 2550 Loss: 800.1868\n",
      "Training...Epoch: 2600 Loss: 796.2715\n",
      "Training...Epoch: 2650 Loss: 792.38794\n",
      "Training...Epoch: 2700 Loss: 788.51935\n",
      "Training...Epoch: 2750 Loss: 784.66565\n",
      "Training...Epoch: 2800 Loss: 780.79504\n",
      "Training...Epoch: 2850 Loss: 776.997\n",
      "Training...Epoch: 2900 Loss: 773.18677\n",
      "Training...Epoch: 2950 Loss: 769.4097\n",
      "Training...Epoch: 3000 Loss: 765.61017\n",
      "-----------------------------\n",
      "Testing...Epoch: 3000 Loss: 1195.5895\n",
      "-----------------------------\n",
      "Training...Epoch: 3050 Loss: 761.81067\n",
      "Training...Epoch: 3100 Loss: 758.05835\n",
      "Training...Epoch: 3150 Loss: 754.3917\n",
      "Training...Epoch: 3200 Loss: 750.7226\n",
      "Training...Epoch: 3250 Loss: 747.18097\n",
      "Training...Epoch: 3300 Loss: 743.63245\n",
      "Training...Epoch: 3350 Loss: 740.07306\n",
      "Training...Epoch: 3400 Loss: 736.5919\n",
      "Training...Epoch: 3450 Loss: 733.06885\n",
      "Training...Epoch: 3500 Loss: 729.4341\n",
      "-----------------------------\n",
      "Testing...Epoch: 3500 Loss: 1163.7489\n",
      "-----------------------------\n",
      "Training...Epoch: 3550 Loss: 725.9623\n",
      "Training...Epoch: 3600 Loss: 722.3437\n",
      "Training...Epoch: 3650 Loss: 717.25836\n",
      "Training...Epoch: 3700 Loss: 713.68787\n",
      "Training...Epoch: 3750 Loss: 710.7384\n",
      "Training...Epoch: 3800 Loss: 707.86725\n",
      "Training...Epoch: 3850 Loss: 705.06006\n",
      "Training...Epoch: 3900 Loss: 702.3081\n",
      "Training...Epoch: 3950 Loss: 699.6269\n",
      "Training...Epoch: 4000 Loss: 697.0127\n",
      "-----------------------------\n",
      "Testing...Epoch: 4000 Loss: 1134.7068\n",
      "-----------------------------\n",
      "Training...Epoch: 4050 Loss: 694.5334\n",
      "Training...Epoch: 4100 Loss: 691.9258\n",
      "Training...Epoch: 4150 Loss: 689.52563\n",
      "Training...Epoch: 4200 Loss: 687.20593\n",
      "Training...Epoch: 4250 Loss: 684.97174\n",
      "Training...Epoch: 4300 Loss: 682.81714\n",
      "Training...Epoch: 4350 Loss: 680.73413\n",
      "Training...Epoch: 4400 Loss: 678.74365\n",
      "Training...Epoch: 4450 Loss: 676.82166\n",
      "Training...Epoch: 4500 Loss: 674.98706\n",
      "-----------------------------\n",
      "Testing...Epoch: 4500 Loss: 1109.6646\n",
      "-----------------------------\n",
      "Training...Epoch: 4550 Loss: 673.36774\n",
      "Training...Epoch: 4600 Loss: 671.5095\n",
      "Training...Epoch: 4650 Loss: 669.92267\n",
      "Training...Epoch: 4700 Loss: 668.39276\n",
      "Training...Epoch: 4750 Loss: 666.9458\n",
      "Training...Epoch: 4800 Loss: 665.5914\n",
      "Training...Epoch: 4850 Loss: 664.31586\n",
      "Training...Epoch: 4900 Loss: 663.0686\n",
      "Training...Epoch: 4950 Loss: 661.9122\n",
      "Training...Epoch: 5000 Loss: 660.82275\n",
      "-----------------------------\n",
      "Testing...Epoch: 5000 Loss: 1091.2565\n",
      "-----------------------------\n",
      "Training...Epoch: 5050 Loss: 659.93115\n",
      "Training...Epoch: 5100 Loss: 658.8365\n",
      "Training...Epoch: 5150 Loss: 657.9539\n",
      "Training...Epoch: 5200 Loss: 657.131\n",
      "Training...Epoch: 5250 Loss: 656.3654\n",
      "Training...Epoch: 5300 Loss: 655.6453\n",
      "Training...Epoch: 5350 Loss: 654.9956\n",
      "Training...Epoch: 5400 Loss: 654.37335\n",
      "Training...Epoch: 5450 Loss: 653.7678\n",
      "Training...Epoch: 5500 Loss: 653.2077\n",
      "-----------------------------\n",
      "Testing...Epoch: 5500 Loss: 1079.8093\n",
      "-----------------------------\n",
      "Training...Epoch: 5550 Loss: 652.86475\n",
      "Training...Epoch: 5600 Loss: 652.23975\n",
      "Training...Epoch: 5650 Loss: 651.8147\n",
      "Training...Epoch: 5700 Loss: 651.4353\n",
      "Training...Epoch: 5750 Loss: 651.08136\n",
      "Training...Epoch: 5800 Loss: 650.75323\n",
      "Training...Epoch: 5850 Loss: 650.3917\n",
      "Training...Epoch: 5900 Loss: 650.0267\n",
      "Training...Epoch: 5950 Loss: 649.7003\n",
      "Training...Epoch: 6000 Loss: 649.40283\n",
      "-----------------------------\n",
      "Testing...Epoch: 6000 Loss: 1077.8899\n",
      "-----------------------------\n",
      "Training...Epoch: 6050 Loss: 649.089\n",
      "Training...Epoch: 6100 Loss: 648.5958\n",
      "Training...Epoch: 6150 Loss: 648.3378\n",
      "Training...Epoch: 6200 Loss: 648.1147\n",
      "Training...Epoch: 6250 Loss: 647.9219\n",
      "Training...Epoch: 6300 Loss: 647.73987\n",
      "Training...Epoch: 6350 Loss: 647.578\n",
      "Training...Epoch: 6400 Loss: 647.39246\n",
      "Training...Epoch: 6450 Loss: 648.8753\n",
      "Training...Epoch: 6500 Loss: 647.12946\n",
      "-----------------------------\n",
      "Testing...Epoch: 6500 Loss: 1075.8682\n",
      "-----------------------------\n",
      "Training...Epoch: 6550 Loss: 647.4439\n",
      "Training...Epoch: 6600 Loss: 646.816\n",
      "Training...Epoch: 6650 Loss: 646.699\n",
      "Training...Epoch: 6700 Loss: 646.59766\n",
      "Training...Epoch: 6750 Loss: 646.4985\n",
      "Training...Epoch: 6800 Loss: 646.40436\n",
      "Training...Epoch: 6850 Loss: 646.3195\n",
      "Training...Epoch: 6900 Loss: 646.245\n",
      "Training...Epoch: 6950 Loss: 648.934\n",
      "Training...Epoch: 7000 Loss: 646.1848\n",
      "-----------------------------\n",
      "Testing...Epoch: 7000 Loss: 1076.3983\n",
      "-----------------------------\n",
      "Training...Epoch: 7050 Loss: 646.3921\n",
      "Training...Epoch: 7100 Loss: 645.9622\n",
      "Training...Epoch: 7150 Loss: 645.8981\n",
      "Training...Epoch: 7200 Loss: 645.8471\n",
      "Training...Epoch: 7250 Loss: 645.79425\n",
      "Training...Epoch: 7300 Loss: 645.7541\n",
      "Training...Epoch: 7350 Loss: 647.5157\n",
      "Training...Epoch: 7400 Loss: 645.6604\n",
      "Training...Epoch: 7450 Loss: 648.5258\n",
      "Training...Epoch: 7500 Loss: 645.7129\n",
      "-----------------------------\n",
      "Testing...Epoch: 7500 Loss: 1076.3197\n",
      "-----------------------------\n",
      "Training...Epoch: 7550 Loss: 646.0865\n",
      "Training...Epoch: 7600 Loss: 645.4677\n",
      "Training...Epoch: 7650 Loss: 645.425\n",
      "Training...Epoch: 7700 Loss: 645.3832\n",
      "Training...Epoch: 7750 Loss: 645.33234\n",
      "Training...Epoch: 7800 Loss: 645.29193\n",
      "Training...Epoch: 7850 Loss: 675.1929\n",
      "Training...Epoch: 7900 Loss: 645.3709\n",
      "Training...Epoch: 7950 Loss: 645.0788\n",
      "Training...Epoch: 8000 Loss: 645.08124\n",
      "-----------------------------\n",
      "Testing...Epoch: 8000 Loss: 1074.9705\n",
      "-----------------------------\n",
      "Training...Epoch: 8050 Loss: 646.01294\n",
      "Training...Epoch: 8100 Loss: 644.9593\n",
      "Training...Epoch: 8150 Loss: 644.91144\n",
      "Training...Epoch: 8200 Loss: 644.874\n",
      "Training...Epoch: 8250 Loss: 644.8351\n",
      "Training...Epoch: 8300 Loss: 644.7998\n",
      "Training...Epoch: 8350 Loss: 644.7641\n",
      "Training...Epoch: 8400 Loss: 649.04553\n",
      "Training...Epoch: 8450 Loss: 644.7425\n",
      "Training...Epoch: 8500 Loss: 644.66675\n",
      "-----------------------------\n",
      "Testing...Epoch: 8500 Loss: 1075.3121\n",
      "-----------------------------\n",
      "Training...Epoch: 8550 Loss: 644.6212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...Epoch: 8600 Loss: 644.49536\n",
      "Training...Epoch: 8650 Loss: 644.46265\n",
      "Training...Epoch: 8700 Loss: 645.9451\n",
      "Training...Epoch: 8750 Loss: 644.47437\n",
      "Training...Epoch: 8800 Loss: 644.3791\n",
      "Training...Epoch: 8850 Loss: 645.26825\n",
      "Training...Epoch: 8900 Loss: 644.4875\n",
      "Training...Epoch: 8950 Loss: 644.2994\n",
      "Training...Epoch: 9000 Loss: 644.2712\n",
      "-----------------------------\n",
      "Testing...Epoch: 9000 Loss: 1075.2711\n",
      "-----------------------------\n",
      "Training...Epoch: 9050 Loss: 644.80347\n",
      "Training...Epoch: 9100 Loss: 644.22107\n",
      "Training...Epoch: 9150 Loss: 644.1935\n",
      "Training...Epoch: 9200 Loss: 644.1673\n",
      "Training...Epoch: 9250 Loss: 644.1426\n",
      "Training...Epoch: 9300 Loss: 644.12366\n",
      "Training...Epoch: 9350 Loss: 646.0694\n",
      "Training...Epoch: 9400 Loss: 644.10065\n",
      "Training...Epoch: 9450 Loss: 644.058\n",
      "Training...Epoch: 9500 Loss: 644.0343\n",
      "-----------------------------\n",
      "Testing...Epoch: 9500 Loss: 1075.2535\n",
      "-----------------------------\n",
      "Training...Epoch: 9550 Loss: 644.2483\n",
      "Training...Epoch: 9600 Loss: 644.0018\n",
      "Training...Epoch: 9650 Loss: 643.9757\n",
      "Training...Epoch: 9700 Loss: 643.955\n",
      "Training...Epoch: 9750 Loss: 643.93353\n",
      "Training...Epoch: 9800 Loss: 643.9128\n",
      "Training...Epoch: 9850 Loss: 711.21857\n",
      "Training...Epoch: 9900 Loss: 643.90155\n",
      "Training...Epoch: 9950 Loss: 643.85004\n",
      "Training...Epoch: 10000 Loss: 643.8273\n",
      "-----------------------------\n",
      "Testing...Epoch: 10000 Loss: 1074.9446\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "Train_losses = []\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "        input_x = Variable(torch.FloatTensor(X_train_standard))  #特征 array -->tensor-->Variable\n",
    "        input_y = Variable(torch.FloatTensor(y_train))  #标签 array -->tensor-->Variable\n",
    "\n",
    "        y_pred = net(input_x)\n",
    "\n",
    "        loss = loss_func(input_y,y_pred)\n",
    "        Train_losses.append(loss.data.numpy())#将Tensor 转成 numpy 数值\n",
    "\n",
    "        #非常标准的一套流程\n",
    "        optimizer.zero_grad() #梯度归0\n",
    "        loss.backward() #求梯度\n",
    "        optimizer.step() #把梯度传进去\n",
    "\n",
    "        #--------------------------#\n",
    "        if(epoch+1)%50 == 0:\n",
    "            print('Training...Epoch:',(epoch+1),'Loss:',loss.data.numpy())\n",
    "        #-------------使用 测试集 去查看 测试集的 效果-----------#\n",
    "        #每隔 500个epoch，就导入 测试集 去查看 模型的效果\n",
    "        if(epoch+1)%500 == 0:\n",
    "                test_x = Variable(torch.FloatTensor(X_test_standard))#特征 array -->tensor-->Variable\n",
    "                test_y = Variable(torch.FloatTensor(y_test))#标签 array -->tensor-->Variable\n",
    "\n",
    "                test_pred = net(test_x)\n",
    "\n",
    "                test_loss = loss_func(test_y,test_pred)\n",
    "\n",
    "                #非常标准的一套流程\n",
    "                optimizer.zero_grad() #梯度归0\n",
    "                test_loss.backward() #求梯度\n",
    "                optimizer.step() #把梯度传进去\n",
    "                print('-----------------------------')\n",
    "                print('Testing...Epoch:',(epoch+1),'Loss:',test_loss.data.numpy())\n",
    "                print('-----------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 让模型能够 收敛 的方法\n",
    "### 1.加大EPOCH\n",
    "### 2.减小学习率\n",
    "### 3.增加模型的复杂度，增加多一些网路层次，增加神经元个数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 验证模型的 好坏，可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.把tensor 转为 array\n",
    "prediction = test_pred.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def caculate(y_true,y_predict):\n",
    "    print''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
